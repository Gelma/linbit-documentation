[[ch-drbdmanage-more]]
== More Information about DRBD Manage

Here you will find some more information about DRBD Manage internals, techniques, and strategies.


[[s-drbdmanage-free-space]]
=== Free Space reporting

DRBD Manage can report free space in two ways:

  * indexterm:[free space]Per node, via `drbdmanage nodes`; this will tell the
    "physical" space available, which might not mean much if using Thin LVs for
    storing data.

  * indexterm:[free space]Via `drbdmanage list-free-space`; this will return
    the size for the single largest volume that can be created with the defined
    replication count.
+
--
So, with 10 storage nodes each having 1TiB free space, the value returned will
be 1TiB, and allocating such a volume will not change the free space value.

For storage nodes with 20GiB, 15GiB, 10GiB, and 5GiB, the free space for
3-way-redundancy will be 10GiB, and 15GiB for 2-way.
--

The free space issue is further muddled quite a bit by thin LVM pools (one or
multiple, depending on storage backend in DRBD Manage - please see
<<s-drbdmanage-storage-plugins>> for more details), and DRBD Manage snapshots.


[[s-drbdmanage-deployment-policy]]
=== Policy plugin, waiting for deployment

When deploying a resource (resp. volume), a snapshot, or when resizing
a volume, you might need to trade between performance and availability.

For a small example, if you have 3 storage servers, but one is down because of
a hardware or software upgrade, and you choose to create a new resource that
should keep three copies - what should happen?

  * Should volume creation block until the third node is available again (hours, days, weeks)?
  * Should volume creation just continue? Should it continue even if only one
    storage server is available, even if 3-way redundancy was requested?
  * Should snapshot creation wait, or continue if at least one server was able to create the snapshot?

The more server you have, the more likely it is that at least one of them is
non-operating at any given time; so you need to decide on your policies, and
configure them.

To avoid having to make such decisions in each driver accessing DRBD Managefootnote:[Like
OpenAttic, OpenNebula, <<ch-openstack,Openstack>>, ProxMox, etc.] individually
(duplicating quite a lot of code), we provide one DRBD Manage
pluginfootnote:["Plugin" might not be the best term here; the code is in the
standard DRBD Manage distribution, but it gets called via the
`run_external_plugin` API.] that aims to provide these functionality.

The exact configuration depends on the specific service using DRBD Manage, of
course; but the basic approach is to allow the admin to specify the waiting
behaviour via a JSON blob, which might be given in the configuration via simple
text string (see eg. <<s-openstack-addtl-conf,Policy configuration for OpenStack>>).


The available policies for `WaitForResource` and `WaitForSnapshot` are:

  * (((Policy,count)))`count` will wait
    for an absolute number of deployments; note that these don't have to be in
    syncfootnote:[On thick LVM a 1TiB deployment would take some time for the
    initial sync, and you most probably don't plan to wait for that, right?] but
    just *deployed*.
  * (((Policy,ratio)))`ratio` is similar,
    but uses a value from `0.0` to `1.0`, which gets multiplied with the number of
    _planned_ deployments. So, for a resource that has (diskful) assignments on
    5 servers, and 3 of them are available, a ratio of `0.6` would work.
+
.NOTE
The given _``ratio``_ will be compared using "greater-or-equal"; so passing
`0.5` in does *not* mean "wait for majority", as one-of-two deployments available
would already be sufficient! Use `0.51` to get the intended meaning.

If you pass in more than one policy, _any_ of them is sufficient; so, for
`count=3` and `ratio=0.75`, 3-out-of-5 servers would return `result=true`.


[[s-drbdmanage-deployment-policy-dr-wr]]
==== More detailed information for driver-writers

For people implementing software that accesses DBRDmanage via the DBus API,
here's an example usage (in mostly Python syntax):

    result, data = run_external_plugin(
                        "drbdmanage.plugins.plugins.wait_for.WaitForResource",
                        { "resource": "foo",
                          "starttime": <unix-timestamp, ie. time(NULL), of first call>
                          "count": "3",
                          "timeout": "15"})

would return the normal array of success/error datafootnote:[Which will always
mean some calling/internal error; "Policy not satisfied" or "timeout" will be
reported _on this level_ as "completed successfully"!], and an additional
dict with detailed status data:

    {
        "policy": "count",
        "result": "true",
        "timeout": "false"
    }

If at least 3 servers had the given resource deployed (at the time of the call), `result` got
`"true"` because the `count` policy was satisfied.


You can wait with/for these plugins/events:

  * (((Policy,resource creation)))`drbdmanage.plugins.plugins.wait_for.WaitForResource`
    needs a `resource` key as input data;
  * (((Policy,snapshot creation)))`drbdmanage.plugins.plugins.wait_for.WaitForSnapshot`
    needs a `resource` and a `snapshot` key as input data;
  * (((Policy,resizing volumes)))`drbdmanage.plugins.plugins.wait_for.WaitForVolumeSize`
    requires `resource`, `volnr`, and `req_size` (net size, in KiB) input arguments.


Please note that because of DBus timeouts this API function does *not* block;
it will return immediately. That's the reason for the `starttime` and
`timeout` values in the input dictionary; if the plugin sees that the
current time is after `starttime` plus `timeout`, it will set the
`timeout` result to `"true"`, so that the driver knows to abort and return
an error.
