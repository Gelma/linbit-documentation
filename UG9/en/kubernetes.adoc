[[ch-kubernetes]]
== LINSTOR Volumes in Kubernetes

indexterm:[Kubernetes]This chapter describes the usage of LINSTOR in Kubernetes
as managed by the operator and with volumes provisioned using the
https://github.com/LINBIT/linstor-csi[LINSTOR CSI plugin].

[[s-kubernetes-overview]]
=== Kubernetes Overview

_Kubernetes_ is a container orchestrator. Kubernetes defines the behavior of
containers and related services via declarative specifications. In this guide,
we'll focus on using `kubectl` to manipulate `.yaml` files that define the
specifications of Kubernetes objects.

[[s-kubernetes-deploy]]
=== Deploying LINSTOR on Kubernetes

[[s-kubernetes-deploy-linstor-operator]]
==== Deploying with the LINSTOR Operator

LINBIT provides a LINSTOR operator to commercial support customers.
The operator eases deployment of LINSTOR on Kubernetes by installing DRBD,
managing Satellite and Controller pods, and other related functions.

The operator itself is installed using a Helm v3 chart as follows:

* Label the worker nodes with `linstor.linbit.com/linstor-node=true`
by running:
+
----
kubectl label node <NODE_NAME> linstor.linbit.com/linstor-node=true
----

* Create a kubernetes secret containing your my.linbit.com credentials:
+
----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>
----
+
The name of this secret must match the one specified in the Helm values,
by default `drbdiocred`.

* Configure storage for the LINSTOR etcd instance. There are various options
for configuring the etcd instance for LINSTOR:
** Use an existing storage provisioner with a default `StorageClass`.
** <<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>.
** Disable persistence for basic testing. This can be done by adding `--set
etcd.persistence.enabled=false` to the `helm install` command below.

* Configuring the storage pools to be created. Storage pools can be
created directly with the helm deployment or at a later time.
<<s-kubernetes-storage-pool-configuration,Learn more about it here>>.
If you do not configure anything, no storage pools will be created.

* Select the appropriate kernel module injector using `--set` with the `helm
install` command in the final step.

** Choose the injector according to the distribution you are using.
Select the latest version from one of `drbd9-rhel7`, `drbd9-rhel8` or `drbd9-bionic` from http://drbd.io/ as appropriate.
The drbd9-rhel8 image should also be used for RHCOS (OpenShift). For example:
+
----
operator.nodeSet.kernelModImage=drbd.io/drbd9-rhel8:v9.0.22-2
----

** Disable kernel module injection if you are installing DRBD by other means.
+
----
operator.nodeSet.drbdKernelModuleInjectionMode=None
----

* Finally create a Helm deployment named `linstor-op` that will set up
everything.
+
----
helm repo add linstor https://charts.linstor.io
helm install linstor-op linstor/linstor
----

[[s-kubernetes-etcd-hostpath-persistence]]
===== LINSTOR etcd `hostPath` persistence

You can use the `pv-hostpath` Helm templates to create `hostPath` persistent
volumes. Create as many PVs as needed to satisfy your configured etcd
`replicaCount` (default 3).

Create the `hostPath` persistent volumes, substituting cluster node
names accordingly in the `nodes=` option:

----
helm repo add linstor https://charts.linstor.io
helm install linstor-etcd linstor/pv-hostpath --set "nodes={<NODE0>,<NODE1>,<NODE2>}"
----

Persistence for etcd is enabled by default. The
`etcd.volumePermissions.enabled` key in the Helm values is also set so that the
`hostPath` volumes have appropriate permissions.

[[s-kubernetes-existing-database]]
===== Using an existing database

LINSTOR can connect to an existing PostgreSQL, MariaDB or etcd database. For
instance, for a PostgreSQL instance with the following configuration:

----
POSTGRES_DB: postgresdb
POSTGRES_USER: postgresadmin
POSTGRES_PASSWORD: admin123
----

The Helm chart can be configured to use this database instead of deploying an
etcd cluster by adding the following to the Helm install command:

----
--set etcd.enabled=false --set "operator.controllerSet.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123"
----

[[s-kubernetes-storage-pool-configuration]]
===== Configuring storage pools

The operator installed by helm can be used to create storage pools. Creation is under control of the
LinstorNodeSet resource:

[source,yaml]
----
$ kubectl get LinstorNodeSet <nodeset> -o yaml
kind: LinstorNodeSet
metadata:
..
spec:
  ..
  storagePools:
    lvmPools:
    - name: lvm-thick
      volumeGroup: drbdpool
    lvmThinPools:
    - name: lvm-thin
      thinVolume: thinpool
      volumeGroup: drbdpool
----

There are two ways to configure storage pools

[[s-kubernetes-storage-pool-configuration-at-install-time]]
====== At install time

At install time, by setting the value of `operator.nodeSet.storagePools` when running helm install.

First create a file with the storage configuration like:

[source,yaml]
----
operator:
  nodeSet:
    storagePools:
      lvmPools:
      - name: lvm-thick
        volumeGroup: drbdpool
      ..
----

This file can be passed to the helm installation like this:

----
helm install linstor-op linstor/linstor -f <file> ..
----

[[s-kubernetes-storage-pool-configuration-after-install]]
====== After install

On a cluster with the operator already configured (i.e. after `helm install`),
you can edit the nodeset configuration like this:

----
$ kubectl edit LinstorNodeSet <nodeset>
----

The storage pool configuration can be updated like in the example above.

[[s-kubernetes-helm-terminate]]
===== Terminating Helm deployment

The LINSTOR deployment can be terminated with:

----
helm delete linstor-op
----

However due to the Helm’s current policy, the newly created Custom Resource
Definitions named `linstorcontrollerset` and `linstornodeset` will *not* be
deleted by the command. This will also cause the instances of those CRD’s named
`linstor-op-ns` and `linstor-op-cs` to remain running.

To terminate those instances after the `helm delete` command, run

----
kubectl patch linstorcontrollerset linstor-op-cs -p '{"metadata":{"finalizers":[]}}' --type=merge
kubectl patch linstornodeset linstor-op-ns -p '{"metadata":{"finalizers":[]}}' --type=merge
----

After that, all the instances created by the Helm deployment will be
terminated.

More information regarding Helm’s current position on CRD’s can be found
https://helm.sh/docs/topics/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you[here].

[[s-kubernetes-deploy-piraeus-operator]]
==== Deploying with the Piraeus Operator

The community supported edition of the LINSTOR deployment in Kubernetes is
called Piraeus. The Piraeus project provides
https://github.com/piraeusdatastore/piraeus-operator[an operator] for
deployment.

[[s-kubernetes-linstor-interacting]]
=== Interacting with LINSTOR in Kubernetes

The Controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.
For instance:

----
kubectl exec linstor-op-cs-controller-0 -- linstor storage-pool list
----

This should only be necessary for investigating problems and accessing advanced functionality.
Regular operation such as creating volumes should be achieved via the
<<s-kubernetes-basic-configuration-and-deployment,Kubernetes integration>>.

[[s-kubernetes-linstor-csi-plugin-deployment]]
=== LINSTOR CSI Plugin Deployment

The operator Helm chart deploys the LINSTOR CSI plugin for you so if you used
that, you can skip this section.

If you are integrating LINSTOR using a different method, you will need to install the LINSTOR CSI plugin.
Instructions for deploying the CSI plugin can be found on the
https://github.com/LINBIT/linstor-csi[project's github]. This will result in a
linstor-csi-controller _StatefulSet_ and a linstor-csi-node _DaemonSet_ running in the
kube-system namespace.

----
NAME                       READY   STATUS    RESTARTS   AGE     IP              NODE
linstor-csi-controller-0   5/5     Running   0          3h10m   191.168.1.200   kubelet-a
linstor-csi-node-4fcnn     2/2     Running   0          3h10m   192.168.1.202   kubelet-c
linstor-csi-node-f2dr7     2/2     Running   0          3h10m   192.168.1.203   kubelet-d
linstor-csi-node-j66bc     2/2     Running   0          3h10m   192.168.1.201   kubelet-b
linstor-csi-node-qb7fw     2/2     Running   0          3h10m   192.168.1.200   kubelet-a
linstor-csi-node-zr75z     2/2     Running   0          3h10m   192.168.1.204   kubelet-e
----

[[s-kubernetes-basic-configuration-and-deployment]]
=== Basic Configuration and Deployment

Once all linstor-csi __Pod__s are up and running, we can provision volumes
using the usual Kubernetes workflows.

Configuring the behavior and properties of LINSTOR volumes deployed via Kubernetes
is accomplished via the use of __StorageClass__es.

IMPORTANT: the "resourceGroup" parameter is mandatory. Usually you want it to be unique and the same as the storage class name.

Here below is the simplest practical _StorageClass_ that can be used to deploy volumes:

.linstor-basic-sc.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  # The name used to identify this StorageClass.
  name: linstor-basic-storage-class
  # The name used to match this StorageClass with a provisioner.
  # linstor.csi.linbit.com is the name that the LINSTOR CSI plugin uses to identify itself
provisioner: linstor.csi.linbit.com
parameters:
  # LINSTOR will provision volumes from the drbdpool storage pool configured
  # On the satellite nodes in the LINSTOR cluster specified in the plugin's deployment
  storagePool: "drbdpool"
  resourceGroup: "linstor-basic-storage-class"
----

DRBD options can be set as well in the parameters section. Valid keys are defined in the
https://app.swaggerhub.com/apis-docs/Linstor/Linstor[LINSTOR REST-API]
(e.g., `DrbdOptions/Net/allow-two-primaries: "yes"`).

We can create the _StorageClass_ with the following command:

----
kubectl create -f linstor-basic-sc.yaml
----

Now that our _StorageClass_ is created, we can now create a _PersistentVolumeClaim_
which can be used to provision volumes known both to Kubernetes and LINSTOR:

.my-first-linstor-volume-pvc.yaml
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-first-linstor-volume
  annotations:
    # This line matches the PersistentVolumeClaim with our StorageClass
    # and therefore our provisioner.
    volume.beta.kubernetes.io/storage-class: linstor-basic-storage-class
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

We can create the _PersistentVolumeClaim_ with the following command:

----
kubectl create -f my-first-linstor-volume-pvc.yaml
----

This will create a _PersistentVolumeClaim_ known to Kubernetes, which will have
a _PersistentVolume_ bound to it, additionally LINSTOR will now create this
volume according to the configuration defined in the `linstor-basic-storage-class`
_StorageClass_. The LINSTOR volume's name will be a UUID prefixed with `csi-`
This volume can be observed with the usual `linstor resource list`. Once that
volume is created, we can attach it to a pod. The following _Pod_ spec will spawn
a Fedora container with our volume attached that busy waits so it is not
unscheduled before we can interact with it:

.my-first-linstor-volume-pod.yaml
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fedora
  namespace: default
spec:
  containers:
  - name: fedora
    image: fedora
    command: [/bin/bash]
    args: ["-c", "while true; do sleep 10; done"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "my-first-linstor-volume"
----

We can create the _Pod_ with the following command:

----
kubectl create -f my-first-linstor-volume-pod.yaml
----

Running `kubectl describe pod fedora` can be used to confirm that _Pod_
scheduling and volume attachment succeeded.

To remove a volume, please ensure that no pod is using it and then delete the
_PersistentVolumeClaim_ via `kubectl`. For example, to remove the volume that we
just made, run the following two commands, noting that the _Pod_ must be
unscheduled before the _PersistentVolumeClaim_ will be removed:

----
kubectl delete pod fedora # unschedule the pod.

kubectl get pod -w # wait for pod to be unscheduled

kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.
----

[[s-kubernetes-snapshots]]
=== Snapshots

Creating <<s-linstor-snapshots, snapshots>> and creating new volumes from
snapshots is done via the use of __VolumeSnapshot__s, __VolumeSnapshotClass__es,
and __PVC__s. First, you'll need to create a _VolumeSnapshotClass_:

.my-first-linstor-snapshot-class.yaml
[source,yaml]
----
kind: VolumeSnapshotClass
apiVersion: snapshot.storage.k8s.io/v1alpha1
metadata:
  name: my-first-linstor-snapshot-class
  namespace: kube-system
snapshotter: io.drbd.linstor-csi
----

Create the _VolumeSnapshotClass_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot-class.yaml
----

Now we will create a volume snapshot for the volume that we created above. This
is done with a _VolumeSnapshot_:

.my-first-linstor-snapshot.yaml
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  name: my-first-linstor-snapshot
spec:
  snapshotClassName: my-first-linstor-snapshot-class
  source:
    name: my-first-linstor-volume
    kind: PersistentVolumeClaim
----

Create the _VolumeSnapshot_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot.yaml
----

Finally, we'll create a new volume from the snapshot with a _PVC_.

.my-first-linstor-volume-from-snapshot.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-first-linstor-volume-from-snapshot
spec:
  storageClassName: linstor-basic-storage-class
  dataSource:
    name: my-first-linstor-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

Create the _PVC_ with `kubectl`:

----
kubectl create -f my-first-linstor-volume-from-snapshot.yaml
----


[[s-kubernetes-volume-accessibility]]
=== Volume Accessibility
// This only covers DRBD volumes, section might change if linked docs are updated.
LINSTOR volumes are typically accessible both locally and
<<s-drbd_clients,over the network>>.

By default, the CSI plugin will attach volumes directly if the _Pod_ happens
to be scheduled on a _kubelet_ where its underlying storage is present. However,
_Pod_ scheduling does not currently take volume locality into account. The
<<s-kubernetes-replicasonsame,replicasOnSame>> parameter can be used to restrict
where the underlying storage may be provisioned, if locally attached volumes
are desired.

See <<s-kubernetes-localstoragepolicy,localStoragePolicy>> to see how this
default behavior can be modified.

[[s-kubernetes-stork]]
=== Volume Locality Optimization using Stork

Stork is a scheduler extender plugin for Kubernetes which allows a storage
driver to give the Kubernetes scheduler hints about where to place a new pod
so that it is optimally located for storage performance. You can learn more
about the project on its https://portworx.com/stork-storage-orchestration-kubernetes/[GitHub page].

We are currently working with the maintainers behind Stork to have a LINSTOR
driver shipped with it by default. In the meantime, you can use a custom-built
Stork container by LINBIT which includes a LINSTOR driver,
https://hub.docker.com/repository/docker/linbit/stork[available on Docker Hub]

==== Deploying Stork alongside the LINSTOR Operator

In order to make Stork talk to LINSTOR, we need to instantiate a few components
in our Kubernetes cluster. First, we need to create a Stork deployment.
This is easiest done with a yaml file supplied by the Stork project:
https://github.com/libopenstorage/stork/blob/master/specs/stork-deployment.yaml[stork-deployment.yaml]

However, a few things need to be adjusted for this deployment to work with LINSTOR.
Specifically, there is this section within the _Deployment_ part:

----
spec:
      containers:
      - command:
        - /stork
        - --driver=pxd
        - --verbose
        - --leader-elect=true
        # Uncomment the line below if you want to enable the feature to
        # automatically update schedulerName
        #- --app-initializer=true
        imagePullPolicy: Always
        image: openstorage/stork:2.2.4
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        name: stork
----

Here we need to change the `--driver=pxd` parameter, which chooses the Portworx
driver by default, to use the LINSTOR driver instead: `--driver=linstor`

A few lines below, `image: openstorage/stork:2.2.4` specifies that the vanilla
Stork image should be used; however, since we want to use LINBIT's image with
the included LINSTOR driver instead, we change it to `image: linbit/stork:latest`

Now we can deploy stork:

----
kubectl create -f stork-deployment.yaml
----

This should start a few Stork pods:

----
$ kubectl get pods --namespace kube-system
NAME                                                READY   STATUS    RESTARTS   AGE
...
stork-58ffcc4844-9xmlc                              1/1     Running   0          16m
stork-58ffcc4844-nh9pb                              1/1     Running   0          16m
stork-58ffcc4844-zbvns                              1/1     Running   0          16m
...
----

Once these pods are up and running, we can start a new Kubernetes scheduler
instance which uses Stork to make its scheduling decisions. There is also another
possibility to accomplish this -- configuring the default scheduler to work
with Stork -- which will not be covered in this guide. Please refer to
https://github.com/libopenstorage/stork#run-stork-in-your-kubernetes-cluster[the upstream Stork documentation]
for more information on this process.

Stork provides another yaml file to start a scheduler instance:
https://github.com/libopenstorage/stork/blob/master/specs/stork-scheduler.yaml[stork-scheduler.yaml].

Again, we need to slightly modify the file for it to work. More precisely, we
need to specify which version of the Kubernetes scheduler we want to deploy.
Find this section near the bottom of the file:

----
spec:
      containers:
      - command:
        - /usr/local/bin/kube-scheduler
        - --address=0.0.0.0
        - --leader-elect=true
        - --scheduler-name=stork
        - --policy-configmap=stork-config
        - --policy-configmap-namespace=kube-system
        - --lock-object-name=stork-scheduler
        image: gcr.io/google_containers/kube-scheduler-amd64:<kube_version>
----

We need to substitute `<kube_version>` for the Kubernetes version our cluster
is running.

----
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"15+", GitVersion:"v1.15.8-beta.0", GitCommit:"6c143d35bb11d74970e7bc0b6c45b6bfdffc0bd4", GitTreeState:"archive", BuildDate:"2020-01-29T00:00:00Z", GoVersion:"go1.14beta1", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.8", GitCommit:"ec6eb119b81be488b030e849b9e64fda4caaf33c", GitTreeState:"clean", BuildDate:"2020-03-12T20:52:22Z", GoVersion:"go1.13.8", Compiler:"gc", Platform:"linux/amd64"}
----

In this case, we can see that under _Server Version_ the `GitVersion` is `v1.16.8`,
so we would replace this line with:

----
        image: gcr.io/google_containers/kube-scheduler-amd64:v1.16.8
----

Now we can create the scheduler deployment:

----
kubectl create -f stork-scheduler.yaml
----

This should have spawned a few more pods:

----
$ kubectl get pods --namespace kube-system
NAME                                                READY   STATUS    RESTARTS   AGE
...
stork-scheduler-f7888f8f5-6djfw                     1/1     Running   0          12m
stork-scheduler-f7888f8f5-jlrrs                     1/1     Running   0          12m
stork-scheduler-f7888f8f5-nqqhz                     1/1     Running   0          12m
...
----

Now that the scheduler is running, we can create a pod which uses it by specifying
the scheduler's name under `spec.schedulerName`.

.my-first-stork-pod.yaml
----
apiVersion: v1
kind: Pod
metadata:
  name: fedora
  namespace: default
spec:
  schedulerName: stork
  containers:
  - name: fedora
    image: fedora
    command: [/bin/bash]
    args: ["-c", "while true; do sleep 10; done"]
    volumeMounts:
    - name: linstor-volume-pvc
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: linstor-volume-pvc
    persistentVolumeClaim:
      claimName: "linstor-volume-pvc"
----

Now the scheduler will make sure to place the pod on a node which has fast
access to `linstor-volume-pvc`.

==== Configuring the Stork LINSTOR Driver

The LINSTOR driver included in Stork can be parameterized to change how Stork
connects to the LINSTOR controller.

This can be accomplished by setting certain environment variables for the
Stork container in its deployment.

.stork-deployment.yaml
----
apiVersion: apps/v1
kind: Deployment
# ...
spec:
  # ...
  template:
    # ...
    spec:
      containers:
      - command:
        - /stork
        - --driver=linstor
        - --verbose
        - --leader-elect=true
        imagePullPolicy: Always
        image: linbit/stork:latest
        resources:
          requests:
            cpu: '0.1'
        securityContext:
          privileged: false
        name: stork
        env:
        - name: LS_ENDPOINT
          value: https://linstor-controller:3371
        - name: LS_USER_CERTIFICATE
          valueFrom:
            secretKeyRef:
              key: tls.crt
              name: linstor-client-tls
        - name: LS_USER_KEY
          valueFrom:
            secretKeyRef:
              key: tls.key
              name: linstor-client-tls
        - name: LS_ROOT_CA
          valueFrom:
            secretKeyRef:
              key: ca.crt
              name: linstor-client-tls
----

The `LS_ENDPOINT` variable can be set to an URL where the LINSTOR controller
can be found. If this is not set, Stork defaults to looking for an endpoint
called `linstor-op-cs` and connects to it.

`LS_USER_CERTIFICATE`, `LS_USER_KEY`, and `LS_ROOT_CA` can be used to configure
encrypted communication with the LINSTOR controller using TLS.

[[s-kubernetes-advanced-configuration]]
=== Advanced Configuration

In general, all configuration for LINSTOR volumes in Kubernetes should be done
via the _StorageClass_ parameters, as seen with the _storagePool_ in the basic
example above. We'll give all the available options an in-depth treatment here.

[[s-kubernetes-nodelist]]
==== nodeList

`nodeList` is a list of nodes for volumes to be assigned to. This will assign
the volume to each node and it will be replicated among all of them. This
can also be used to select a single node by hostname, but it's more flexible to use
<<s-kubernetes-replicasonsame,replicasOnSame>> to select a single node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-autoplace,autoPlace>>.

TIP: This option determines on which LINSTOR nodes the underlying storage for volumes
will be provisioned and is orthogonal from which _kubelets_ these volumes will be
accessible.

Example: `nodeList: "node-a node-b node-c"`

Example: `nodeList: "node-a"`

[[s-kubernetes-autoplace]]
==== autoPlace

`autoPlace` is an integer that determines the amount of replicas a volume of
this _StorageClass_ will have.  For instance, `autoPlace: 3` will produce
volumes with three-way replication. If neither `autoPlace` nor `nodeList` are
set, volumes will be <<s-autoplace-linstor,automatically placed>> on one node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-nodelist,nodeList>>.

TIP: This option (and all options which affect autoplacement behavior) modifies the
number of LINSTOR nodes on which the underlying storage for volumes will be
provisioned and is orthogonal to which _kubelets_ those volumes will be accessible
from.

Example: `autoPlace: 2`

Default: `autoPlace: 1`


[[s-kubernetes-replicasonsame]]
==== replicasOnSame

// These should link to the linstor documentation about node properties, but those
// do not exist at the time of this commit.
`replicasOnSame` is a list of key=value pairs used as required autoplacement selection
labels when <<s-kubernetes-autoplace,autoplace>> is used to determine where to
provision storage. These labels correspond to LINSTOR node aux props. Please note both
the key and value names are user-defined and arbitrary. Let's explore this behavior
with examples assuming a LINSTOR cluster such that `node-a` is configured with the
following aux props `zone=z1` and `role=backups`, while `node-b` is configured with
only `zone=z1`.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1 role=backups"`,
then all volumes created from that _StorageClass_ will be provisioned on `node-a`,
since that is the only node with all of the correct key=value pairs in the LINSTOR
cluster. This is the most flexible way to select a single node for provisioning.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on either `node-a` or `node-b` as they both have
the `zone=z1` aux prop.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1 role=backups"`,
then provisioning will fail, as there are not two or more nodes that have
the appropriate aux props.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on both `node-a` and `node-b` as they both have
the `zone=z1` aux prop.

Example: `replicasOnSame: "zone=z1 role=backups"`

[[s-kubernetes-replicasondifferent]]
==== replicasOnDifferent

`replicasOnDifferent` is a list of key=value pairs to avoid as autoplacement
selection. It is the inverse of <<s-kubernetes-replicasonsame,replicasOnSame>>.

Example: `replicasOnDifferent: "no-csi-volumes=true"`

[[s-kubernetes-localstoragepolicy]]
==== localStoragePolicy

`localStoragePolicy` determines, via volume topology, which LINSTOR
__Satellite__s volumes should be assigned and from where Kubernetes will
access volumes. The behavior of each option is explained below in detail.

IMPORTANT: If you specify a <<s-kubernetes-nodelist,nodeList>>, volumes will
be created on those nodes, irrespective of the `localStoragePolicy`; however,
the accessibility reporting will still be as described.

IMPORTANT: You must set `volumeBindingMode: WaitForFirstConsumer` in the
_StorageClass_ and the LINSTOR __Satellite__s running on the __kubelet__s must
be able to support the diskfull placement of volumes as they are configured in
that _StorageClass_ for <<s-kubernetes-localstoragepolicy-required,required>>
or <<s-kubernetes-localstoragepolicy-preferred,preferred>> to work properly.

TIP: Use `topologyKey: "linbit.com/hostname"` rather than `topologyKey:
"kubernetes.io/hostname"` if you are setting `affinity` in your _Pod_ or
_StatefulSet_ specs.

Example: `localStoragePolicy: required`

[[s-kubernetes-localstoragepolicy-ignore]]
===== ignore (default)

When `localStoragePolicy` is set to `ignore`, regular autoplacement
occurs based on <<s-kubernetes-autoplace,autoplace>>,
<<s-kubernetes-replicasonsame,replicasOnSame>>, and
<<s-kubernetes-replicasonsame,replicasOnDifferent>>. Volume location will not
affect _Pod_ scheduling in Kubernetes and the volumes will be accessed over
the network if they're not local to the _kubelet_ where the _Pod_ was scheduled.

[[s-kubernetes-localstoragepolicy-required]]
===== required

When `localStoragePolicy` is set to `required`, Kubernetes will report a list
of places that it wants to schedule a _Pod_ in order of preference. The plugin
will attempt to provision the volume(s) according to that preference. The
number of volumes to be provisioned in total is based off of
<<s-kubernetes-autoplace,autoplace>>.

If all preferences have been attempted, but no volumes where successfully
assigned volume creation will fail.

In case of multiple replicas when all preferences have been attempted, and at
least one has succeeded, but there are still replicas remaining to be
provisioned, <<s-kubernetes-autoplace,autoplace>> behavior will apply for the
remaining volumes.

With this option set, Kubernetes will consider volumes that are not locally
present on a _kubelet_ to be unaccessible from that _kubelet_.

[[s-kubernetes-localstoragepolicy-preferred]]
===== preferred

When `localStoragePolicy` is set to `preferred`, volume placement behavior
will be the same as when it's set to
<<s-kubernetes-localstoragepolicy-required,required>> with the exception that
volume creation will not fail if no preference was able to be satisfied.
Volume accessibility will be the same as when set to
<<s-kubernetes-localstoragepolicy-ignore,ignore>>.

[[s-kubernetes-storagepool]]
==== storagePool

`storagePool` is the name of the LINSTOR <<s-storage_pools,storage pool>> that
will be used to provide storage to the newly-created volumes.

CAUTION: Only nodes configured with this same _storage pool_ with be considered
for <<s-kubernetes-autoplace,autoplacement>>. Likewise, for _StorageClasses_ using
<<s-kubernetes-nodelist,nodeList>> all nodes specified in that list must have this
_storage pool_ configured on them.

Example: `storagePool: my-storage-pool`

[[s-kubernetes-disklessstoragepool]]
==== disklessStoragePool

// This should link to the linstor section talking about diskless storage pools
// when that gets written.
`disklessStoragePool` is an optional parameter that only effects LINSTOR volumes
assigned disklessly to _kubelets_ i.e., as clients. If you have a custom
_diskless storage pool_ defined in LINSTOR, you'll specify that here.

Example: `disklessStoragePool: my-custom-diskless-pool`

[[s-kubernetes-encryption]]
==== encryption

`encryption` is an optional parameter that determines whether to encrypt
volumes. LINSTOR must be <<s-linstor-encrypted-Volumes,configured for encryption>>
for this to work properly.

Example: `encryption: "true"`

[[s-kubernetes-filesystem]]
==== filesystem

`filesystem` is an option parameter to specify the filesystem for non raw block
volumes. Currently supported options are `xfs` and `ext4`.

Example: `filesystem: "xfs"`

Default: `filesystem: "ext4"`

[[s-kubernetes-fsops]]
==== fsOpts
`fsOpts` is an optional parameter that passes options to the volume's
filesystem at creation time.

IMPORTANT: Please note these values are specific to your chosen
<<s-kubernetes-filesystem, filesystem>>.

Example: `fsOpts: "-b 2048"`

[[s-kubernetes-mountops]]
==== mountOpts
`mountOpts` is an optional parameter that passes options to the volume's
filesystem at mount time.

Example: `mountOpts: "sync,noatime"`
